---
title: "Week 5 Lab"
author: "J. Arturo Esquivel"
date: today
date-format: "DD/MM/YY"
format: pdf
execute: 
  warning: false
  message: false
---

```{r}
library(tidybayes)
library(tidyverse)
library(rstan)
library(here)
```

```{r}
kidiq <- read_rds(here("kidiq.RDS"))
```

# Question 1

First we show the kids' scores distribution. They seem to approach a normal distribution, centered around a score of 95. There also seem to be a couple of extremely low observations, maybe due to measurement problems, or incomplete data.

```{r}
#| message: false
#| warning: false

ggplot(kidiq) + 
  geom_histogram(aes(x = kid_score, y = after_stat(density)), position = 'dodge')  +
  labs(title = "Kid Score Distribution",
       x = "Kid Score",
       y = "Density") + 
  theme_bw(base_size = 13) 
``` 

Then, we can inspect the relationship between kids' scores, and that of their mothers. There seems to be a slight direct relationship between the two. 

```{r}
#| message: false
#| warning: false

kidiq |> 
  ggplot(aes(kid_score, mom_iq)) + 
  geom_point() + 
  geom_smooth() +
  labs(title = "Comparison Between Mother IQ and Kid Score",
       x = "Mom IQ", y = "Kid Score") + 
  theme_bw(base_size = 12)
``` 

Finally, we can assess how kids' IQ relates to High School completion by their mothers. The plot below shows that, overall, kids which mothers completed High School tend to have higher scores. 

```{r}
#| message: false
#| warning: false
ggplot(kidiq, aes(as.factor(mom_hs), kid_score, color = as.factor(mom_hs))) +
  geom_boxplot() + 
  labs(title = "Kid Scores by Mother High School Status",
       x = "Mother Completed High School", y = "Kid Score",
       color = "Completed") + 
  theme_bw(base_size = 13)  +
  scale_color_discrete( labels = c('No', 'Yes'))
```


```{r}
#| message: false
#| warning: false
#| results: hide

y <- kidiq$kid_score
mu0 <- 80
sigma0 <- 10

# named list to input for stan function
data <- list(y = y, 
             N = length(y), 
             mu0 = mu0,
             sigma0 = sigma0)

fit <- stan(file = here("kids2.stan"),
            data = data,
            chains = 3,
            iter = 500,
            verbose = FALSE,
            refresh = 0)
```

# Question 2

The results for such a model are shown below. 

```{r}
#| message: false
#| warning: false
#| results: hide

sigma0 <- 0.1

data <- list(y = y, 
             N = length(y), 
             mu0 = mu0,
             sigma0 = sigma0)

fit_i <- stan(file = here("kids2.stan"),
            data = data,
            chains = 3,
            iter = 500)
```



```{r }
#| message: false
#| warning: false

summary(fit)$summary[,1]
summary(fit_i)$summary[,1]
```

The results do change considerably. Including a highly informative prior leads to a posterior mean virtually equal to the one specified in the prior. The posterior mean went from `r round(mean(extract(fit)[["mu"]]),1)` when the prior was weakly informative, to `r round(mean(extract(fit_i)[["mu"]]),1)` for the highly informative prior. 

Looking at the prior and posterior distributions we can see that the posterior distribution remained pretty much the same as the prior specified. 

```{r}
#| message: false
#| warning: false

dsamples <- fit_i  |> 
  gather_draws(mu, sigma)

dsamples |> 
  filter(.variable == "mu") |> 
  ggplot(aes(.value, color = "posterior")) + geom_density(size = 1) + 
  xlim(c(75, 90)) + 
  stat_function(fun = dnorm, 
        args = list(mean = mu0, 
                    sd = sigma0), 
        aes(colour = 'prior'), size = 1) +
  scale_color_manual(name = "", values = c("prior" = "red", "posterior" = "black")) + 
  ggtitle("Prior and Posterior for Mean Test Scores") + 
  xlab("Score")
```

```{r}
#| message: false
#| warning: false
#| results: hide

X <- as.matrix(kidiq$mom_hs, ncol = 1)
K <- 1

data <- list(y = y, N = length(y), 
             X = X, K = K)
fit2 <- stan(file = here("kids3.stan"),
            data = data, 
            iter = 1000)
```

# Question 3

a) 

We can see that both, the intercept and $\hat\beta$ from the simple linear regression model are similar to the corresponding posterior means from the Bayesian model. 

```{r}
#| message: false
#| warning: false

fit_lm <- lm(kid_score ~ mom_hs, kidiq)

summary(fit2)$summary[,1]
fit_lm$coefficients
```

b) 

The `pairs` plot is shown below. We see that the joint distribution for the parameters follows the line-like pattern common to linear regression. This can only be a problem in terms of efficiency, since exploration across the distribution of the parameters is limited to values along the line pattern. 

```{r}
#| message: false
#| warning: false

pairs(fit2, pars = c("alpha", "beta[1]"))
```

# Question 4

```{r}
#| message: false
#| warning: false
#| results: hide

kidiq$mom_iq_c <- kidiq$mom_iq - mean(kidiq$mom_iq)
X <- as.matrix(cbind(kidiq$mom_hs, kidiq$mom_iq_c), ncol = 2)
K <- 2

data <- list(y = y, N = length(y), 
             X = X, K = K)
fit3 <- stan(file = here("kids4.stan"),
            data = data, 
            iter = 1000)
```

The results for such a model are below. The coefficient for the mother's IQ centered suggests that for every unit the IQ of the mother is above average, the kid's expected score increases in `r round(mean(extract(fit3)[["beta"]][ , 2]), 2)`.

```{r}
#| message: false
#| warning: false

summary(fit3)$summary[,1]
```

# Question 5 

As the summary below shows, the results obtained are vary similar for both approaches. 

```{r}
#| message: false
#| warning: false

fit_lm2 <- lm(kid_score ~ mom_hs + mom_iq_c, kidiq)
fit_lm2$coefficients
```

# Question 6

The plot of the posterior estimates given the education conditions of the mother are shown below. 

```{r }
#| message: false
#| warning: false

estimate0 <- extract(fit3)[["alpha"]] + extract(fit3)[["beta"]][, 2]*(110-mean(kidiq$mom_iq)) 
estimate1 <- extract(fit3)[["alpha"]] + extract(fit3)[["beta"]][, 1] +
  extract(fit3)[["beta"]][, 2]*(110-mean(kidiq$mom_iq)) 

data.frame("No" = estimate0, "Yes" = estimate1) |> 
  pivot_longer(c(No,Yes), names_to = "hs_completed") |>
  ggplot(aes(value, fill = hs_completed)) + 
  geom_histogram()  + 
  labs(title = "Estimates Given Mothers' High Scool Completion",
       x = "Score Estimate", y = "Frequency",
       fill = "HS Completed") + 
  theme_bw(base_size = 12) 

```

# Question 7

The posterior predictive distribution for a new kid with a mother who graduated high school and has an IQ of 95 is shown below. 

```{r }
#| message: false
#| warning: false

prediction <- extract(fit3)[["alpha"]] + 
  extract(fit3)[["beta"]][, 1] +
  extract(fit3)[["beta"]][, 2]*(95-mean(kidiq$mom_iq)) + 
  rnorm(length(extract(fit3)[["alpha"]]), 0, extract(fit3)[["sigma"]])

data.frame("Pred" = prediction) |> 
  ggplot(aes(Pred)) + 
  geom_histogram()  + 
  labs(title = "Posterior Predictive Distribution",
       x = "Predicted Score", y = "Frequency") + 
  theme_bw(base_size = 13) 

```
